{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras as K\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import gym\n",
    "from tensorflow import keras as K\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# OpenAI Gym用に使うNES エミュレーター\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#OpenAI Gymのスーパー・マリオ・ブラザーズの環境\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\",\n",
    "                        [\"s\", \"a\", \"r\", \"n_s\", \"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNAgent():\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "\n",
    "    def save(self, model_path):\n",
    "        self.model.save(model_path, overwrite=True, include_optimizer=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = K.models.load_model(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def initialize(self, experiences):\n",
    "        raise NotImplementedError(\"You have to implement initialize method.\")\n",
    "\n",
    "    def estimate(self, s):\n",
    "        raise NotImplementedError(\"You have to implement estimate method.\")\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        raise NotImplementedError(\"You have to implement update method.\")\n",
    "\n",
    "    def policy(self, s):\n",
    "        #ε-greedy法ね\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(s)\n",
    "            if self.estimate_probs:\n",
    "                action = np.random.choice(self.actions,\n",
    "                                          size=1, p=estimates)[0]\n",
    "                return action\n",
    "            else:\n",
    "                return np.argmax(estimates)\n",
    "\n",
    "    def play(self, env, episode_count=5, render=True):\n",
    "        for e in range(episode_count):\n",
    "            s = env.reset()\n",
    "            s = s[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                a = self.policy(s)\n",
    "                # n_state, reward, done, info = env.step(a)\n",
    "                n_state, reward, terminated, truncated, info = env.step(a)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                s = n_state\n",
    "            else:\n",
    "                print(\"Get reward {}.\".format(episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, buffer_size=1024, batch_size=32,\n",
    "                 gamma=0.9, report_interval=10, log_dir=\"\"):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.logger = Logger(log_dir, self.trainer_name)\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "\n",
    "    @property\n",
    "    def trainer_name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        snaked = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", class_name)\n",
    "        snaked = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", snaked).lower()\n",
    "        snaked = snaked.replace(\"_trainer\", \"\")\n",
    "        return snaked\n",
    "\n",
    "    def train_loop(self, env, agent, episode=200, initial_count=-1,\n",
    "                   render=False, observe_interval=0):\n",
    "        self.experiences = deque(maxlen=self.buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        frames = []\n",
    "\n",
    "        for i in range(episode):\n",
    "            s = env.reset()\n",
    "            s = s[0]\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.episode_begin(i, agent)\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if self.training and observe_interval > 0 and\\\n",
    "                   (self.training_count == 1 or\n",
    "                    self.training_count % observe_interval == 0):\n",
    "                    frames.append(s)\n",
    "\n",
    "                a = agent.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                e = Experience(s, a, reward, n_state, done)\n",
    "                self.experiences.append(e)\n",
    "                if not self.training and \\\n",
    "                   len(self.experiences) == self.buffer_size:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                self.step(i, step_count, agent, e)\n",
    "\n",
    "                s = n_state\n",
    "                step_count += 1\n",
    "            else:\n",
    "                self.episode_end(i, step_count, agent)\n",
    "\n",
    "                if not self.training and \\\n",
    "                   initial_count > 0 and i >= initial_count:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                if self.training:\n",
    "                    if len(frames) > 0:\n",
    "                        self.logger.write_image(self.training_count,\n",
    "                                                frames)\n",
    "                        frames = []\n",
    "                    self.training_count += 1\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        pass\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        pass\n",
    "\n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False\n",
    "\n",
    "    def get_recent(self, count):\n",
    "        recent = range(len(self.experiences) - count, len(self.experiences))\n",
    "        return [self.experiences[i] for i in recent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space\n",
    "\n",
    "    # def reset(self):\n",
    "    #     return self.transform(self._env.reset())\n",
    "\n",
    "    def reset(self):\n",
    "        state = self._env.reset()\n",
    "        if isinstance(state, tuple):  # 新しい形式に対応\n",
    "            state = state[0]  # 観測値のみを取得\n",
    "        return self.transform(state)\n",
    "\n",
    "    def render(self):\n",
    "        self._env.render(mode=\"human\")\n",
    "\n",
    "    def step(self, action):\n",
    "        # n_state, reward, done, info = self._env.step(action)\n",
    "        n_state, reward, terminated, truncated, info = self._env.step(action)\n",
    "        done = terminated or truncated\n",
    "        return self.transform(n_state), reward, done, info\n",
    "\n",
    "    def transform(self, state):\n",
    "        raise NotImplementedError(\"You have to implement transform method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir=\"\", dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "        if not log_dir:\n",
    "            # self.log_dir = os.path.join(os.path.dirname(__file__), \"logs\")\n",
    "            self.log_dir = os.path.join(os.getcwd(), \"logs\") #jupyternotebook用\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.mkdir(self.log_dir)\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.compat.v1.keras.callbacks.TensorBoard(\n",
    "                            self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def path_of(self, file_name):\n",
    "        return os.path.join(self.log_dir, file_name)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.compat.v1.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_image(self, index, frames):\n",
    "        # Deal with a 'frames' as a list of sequential gray scaled image.\n",
    "        last_frames = [f[:, :, -1] for f in frames]\n",
    "        if np.min(last_frames[-1]) < 0:\n",
    "            scale = 127 / np.abs(last_frames[-1]).max()\n",
    "            offset = 128\n",
    "        else:\n",
    "            scale = 255 / np.max(last_frames[-1])\n",
    "            offset = 0\n",
    "        channel = 1  # gray scale\n",
    "        tag = \"frames_at_training_{}\".format(index)\n",
    "        values = []\n",
    "\n",
    "        for f in last_frames:\n",
    "            height, width = f.shape\n",
    "            array = np.asarray(f * scale + offset, dtype=np.uint8)\n",
    "            image = Image.fromarray(array)\n",
    "            output = io.BytesIO()\n",
    "            image.save(output, format=\"PNG\")\n",
    "            image_string = output.getvalue()\n",
    "            output.close()\n",
    "            image = tf.compat.v1.Summary.Image(\n",
    "                        height=height, width=width, colorspace=channel,\n",
    "                        encoded_image_string=image_string)\n",
    "            value = tf.compat.v1.Summary.Value(tag=tag, image=image)\n",
    "            values.append(value)\n",
    "\n",
    "        summary = tf.compat.v1.Summary(value=values)\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkAgent(FNAgent):\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        super().__init__(epsilon, actions)\n",
    "        self._scaler = None\n",
    "        self._teacher_model = None\n",
    "\n",
    "    def initialize(self, experiences, optimizer):\n",
    "        feature_shape = experiences[0].s.shape\n",
    "        self.make_model(feature_shape)\n",
    "        self.model.compile(optimizer, loss=\"mse\")\n",
    "        self.initialized = True\n",
    "        print(\"Done initialization. From now, begin training!\")\n",
    "\n",
    "    def make_model(self, feature_shape):\n",
    "        normal = K.initializers.glorot_normal()\n",
    "        model = K.Sequential()\n",
    "        model.add(K.layers.Conv2D(\n",
    "            32, kernel_size=8, strides=4, padding=\"same\",\n",
    "            input_shape=feature_shape, kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Conv2D(\n",
    "            64, kernel_size=4, strides=2, padding=\"same\",\n",
    "            kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Conv2D(\n",
    "            64, kernel_size=3, strides=1, padding=\"same\",\n",
    "            kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Flatten())\n",
    "        model.add(K.layers.Dense(256, kernel_initializer=normal,\n",
    "                                 activation=\"relu\"))\n",
    "        model.add(K.layers.Dense(len(self.actions),\n",
    "                                 kernel_initializer=normal))\n",
    "        self.model = model\n",
    "        self._teacher_model = K.models.clone_model(self.model)\n",
    "\n",
    "    def estimate(self, state):\n",
    "        return self.model.predict(np.array([state]))[0]\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        states = np.array([e.s for e in experiences])\n",
    "        n_states = np.array([e.n_s for e in experiences])\n",
    "\n",
    "        estimateds = self.model.predict(states)\n",
    "        future = self._teacher_model.predict(n_states)\n",
    "\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                reward += gamma * np.max(future[i])\n",
    "            estimateds[i][e.a] = reward\n",
    "\n",
    "        loss = self.model.train_on_batch(states, estimateds)\n",
    "        return loss\n",
    "\n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkAgentTest(DeepQNetworkAgent):\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        super().__init__(epsilon, actions)\n",
    "\n",
    "    def make_model(self, feature_shape):\n",
    "        normal = K.initializers.glorot_normal()\n",
    "        model = K.Sequential()\n",
    "        model.add(K.layers.Dense(64, input_shape=feature_shape,\n",
    "                                 kernel_initializer=normal, activation=\"relu\"))\n",
    "        model.add(K.layers.Dense(len(self.actions), kernel_initializer=normal,\n",
    "                                 activation=\"relu\"))\n",
    "        self.model = model\n",
    "        self._teacher_model = K.models.clone_model(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatcherObserver(Observer):\n",
    "\n",
    "    def __init__(self, env, width, height, frame_count):\n",
    "        super().__init__(env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.frame_count = frame_count\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def transform(self, state):\n",
    "        grayed = Image.fromarray(state).convert(\"L\")\n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(\"float\")\n",
    "        normalized = resized / 255.0  # scale to 0~1\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        # Convert the feature shape (f, w, h) => (h, w, f).\n",
    "        feature = np.transpose(feature, (1, 2, 0))\n",
    "\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, buffer_size=50000, batch_size=32,\n",
    "                 gamma=0.99, initial_epsilon=0.5, final_epsilon=1e-3,\n",
    "                 learning_rate=1e-3, teacher_update_freq=3, report_interval=10,\n",
    "                 log_dir=\"\", file_name=\"\"):\n",
    "        super().__init__(buffer_size, batch_size, gamma,\n",
    "                         report_interval, log_dir)\n",
    "        self.file_name = file_name if file_name else \"dqn_agent.h5\"\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_update_freq = teacher_update_freq\n",
    "        self.loss = 0\n",
    "        self.training_episode = 0\n",
    "        self._max_reward = -10\n",
    "\n",
    "    def train(self, env, episode_count=1200, initial_count=200,\n",
    "              test_mode=False, render=False, observe_interval=100):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if not test_mode:\n",
    "            agent = DeepQNetworkAgent(1.0, actions)\n",
    "        else:\n",
    "            agent = DeepQNetworkAgentTest(1.0, actions)\n",
    "            observe_interval = 0\n",
    "        self.training_episode = episode_count\n",
    "\n",
    "        self.train_loop(env, agent, episode_count, initial_count, render,\n",
    "                        observe_interval)\n",
    "        return agent\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        self.loss = 0\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        optimizer = K.optimizers.Adam(lr=self.learning_rate, clipvalue=1.0)\n",
    "        agent.initialize(self.experiences, optimizer)\n",
    "        self.logger.set_model(agent.model)\n",
    "        agent.epsilon = self.initial_epsilon\n",
    "        self.training_episode -= episode\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            self.loss += agent.update(batch, self.gamma)\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        reward = sum([e.r for e in self.get_recent(step_count)])\n",
    "        self.loss = self.loss / step_count\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", self.loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.path_of(self.file_name))\n",
    "                self._max_reward = reward\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "\n",
    "            diff = (self.initial_epsilon - self.final_epsilon)\n",
    "            decay = diff / self.training_episode\n",
    "            agent.epsilon = max(agent.epsilon - decay, self.final_epsilon)\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(play, is_test):\n",
    "    file_name = \"dqn_agent.h5\" if not is_test else \"dqn_agent_test.h5\"\n",
    "    trainer = DeepQNetworkTrainer(file_name=file_name)\n",
    "    path = trainer.logger.path_of(trainer.file_name)\n",
    "    agent_class = DeepQNetworkAgent\n",
    "\n",
    "    if is_test:\n",
    "        print(\"Train on test mode\")\n",
    "        obs = gym.make('SuperMarioBros-v1', apply_api_compatibility=True, render_mode=\"human\")\n",
    "        agent_class = DeepQNetworkAgentTest\n",
    "    else:\n",
    "        env = gym.make('SuperMarioBros-v1', apply_api_compatibility=True, render_mode=\"human\")\n",
    "        obs = CatcherObserver(env, 80, 80, 4)\n",
    "        trainer.learning_rate = 1e-4\n",
    "\n",
    "    if play:\n",
    "        agent = agent_class.load(obs, path)\n",
    "        agent.play(obs, render=True)\n",
    "    else:\n",
    "        trainer.train(obs, test_mode=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "2024-07-13 16:50:22.032 Python[9092:87175] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/s_/w4ncchnj77322v1x6dslwsq40000gn/T/org.python.python.savedState\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "2024-07-13 16:50:32.616 Python[9092:87175] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.0001}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 学習モード\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(play, is_test)\u001b[0m\n\u001b[1;32m     18\u001b[0m     agent\u001b[38;5;241m.\u001b[39mplay(obs, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mDeepQNetworkTrainer.train\u001b[0;34m(self, env, episode_count, initial_count, test_mode, render, observe_interval)\u001b[0m\n\u001b[1;32m     25\u001b[0m     observe_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_episode \u001b[38;5;241m=\u001b[39m episode_count\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43mobserve_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mTrainer.train_loop\u001b[0;34m(self, env, agent, episode, initial_count, render, observe_interval)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiences\u001b[38;5;241m.\u001b[39mappend(e)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m     50\u001b[0m    \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiences) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(i, step_count, agent, e)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mDeepQNetworkTrainer.begin_train\u001b[0;34m(self, episode, agent)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbegin_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, episode, agent):\n\u001b[0;32m---> 36\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     agent\u001b[38;5;241m.\u001b[39minitialize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiences, optimizer)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mset_model(agent\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/optimizers/adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     61\u001b[0m ):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:37\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.0001}"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 学習モード\n",
    "main(play=False, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('SuperMarioBros-v1', apply_api_compatibility=True, render_mode=\"human\")\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "# env.reset()\n",
    "# next_state, reward, done, truncated, info = env.step(action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
