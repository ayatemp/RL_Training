{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras as K\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\",\n",
    "                        [\"s\", \"a\", \"r\", \"n_s\", \"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNAgent():\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "\n",
    "    def save(self, model_path):\n",
    "        self.model.save(model_path, overwrite=True, include_optimizer=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = K.models.load_model(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def initialize(self, experiences):\n",
    "        raise NotImplementedError(\"You have to implement initialize method.\")\n",
    "\n",
    "    def estimate(self, s):\n",
    "        raise NotImplementedError(\"You have to implement estimate method.\")\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        raise NotImplementedError(\"You have to implement update method.\")\n",
    "\n",
    "    def policy(self, s):\n",
    "        #ε-greedy法ね\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(s)\n",
    "            if self.estimate_probs:\n",
    "                action = np.random.choice(self.actions,\n",
    "                                          size=1, p=estimates)[0]\n",
    "                return action\n",
    "            else:\n",
    "                return np.argmax(estimates)\n",
    "\n",
    "    def play(self, env, episode_count=5, render=True):\n",
    "        for e in range(episode_count):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                a = self.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                episode_reward += reward\n",
    "                s = n_state\n",
    "            else:\n",
    "                print(\"Get reward {}.\".format(episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, buffer_size=1024, batch_size=32,\n",
    "                 gamma=0.9, report_interval=10, log_dir=\"\"):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.logger = Logger(log_dir, self.trainer_name)\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "\n",
    "    @property\n",
    "    def trainer_name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        snaked = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", class_name)\n",
    "        snaked = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", snaked).lower()\n",
    "        snaked = snaked.replace(\"_trainer\", \"\")\n",
    "        return snaked\n",
    "\n",
    "    def train_loop(self, env, agent, episode=200, initial_count=-1,\n",
    "                   render=False, observe_interval=0):\n",
    "        self.experiences = deque(maxlen=self.buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        frames = []\n",
    "\n",
    "        for i in range(episode):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.episode_begin(i, agent)\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if self.training and observe_interval > 0 and\\\n",
    "                   (self.training_count == 1 or\n",
    "                    self.training_count % observe_interval == 0):\n",
    "                    frames.append(s)\n",
    "\n",
    "                a = agent.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                e = Experience(s, a, reward, n_state, done)\n",
    "                self.experiences.append(e)\n",
    "                if not self.training and \\\n",
    "                   len(self.experiences) == self.buffer_size:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                self.step(i, step_count, agent, e)\n",
    "\n",
    "                s = n_state\n",
    "                step_count += 1\n",
    "            else:\n",
    "                self.episode_end(i, step_count, agent)\n",
    "\n",
    "                if not self.training and \\\n",
    "                   initial_count > 0 and i >= initial_count:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                if self.training:\n",
    "                    if len(frames) > 0:\n",
    "                        self.logger.write_image(self.training_count,\n",
    "                                                frames)\n",
    "                        frames = []\n",
    "                    self.training_count += 1\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        pass\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        pass\n",
    "\n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False\n",
    "\n",
    "    def get_recent(self, count):\n",
    "        recent = range(len(self.experiences) - count, len(self.experiences))\n",
    "        return [self.experiences[i] for i in recent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "\n",
    "    def render(self):\n",
    "        self._env.render(mode=\"human\")\n",
    "\n",
    "    def step(self, action):\n",
    "        n_state, reward, done, info = self._env.step(action)\n",
    "        return self.transform(n_state), reward, done, info\n",
    "\n",
    "    def transform(self, state):\n",
    "        raise NotImplementedError(\"You have to implement transform method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir=\"\", dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "        if not log_dir:\n",
    "            self.log_dir = os.path.join(os.path.dirname(__file__), \"logs\")\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.mkdir(self.log_dir)\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.compat.v1.keras.callbacks.TensorBoard(\n",
    "                            self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def path_of(self, file_name):\n",
    "        return os.path.join(self.log_dir, file_name)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.compat.v1.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_image(self, index, frames):\n",
    "        # Deal with a 'frames' as a list of sequential gray scaled image.\n",
    "        last_frames = [f[:, :, -1] for f in frames]\n",
    "        if np.min(last_frames[-1]) < 0:\n",
    "            scale = 127 / np.abs(last_frames[-1]).max()\n",
    "            offset = 128\n",
    "        else:\n",
    "            scale = 255 / np.max(last_frames[-1])\n",
    "            offset = 0\n",
    "        channel = 1  # gray scale\n",
    "        tag = \"frames_at_training_{}\".format(index)\n",
    "        values = []\n",
    "\n",
    "        for f in last_frames:\n",
    "            height, width = f.shape\n",
    "            array = np.asarray(f * scale + offset, dtype=np.uint8)\n",
    "            image = Image.fromarray(array)\n",
    "            output = io.BytesIO()\n",
    "            image.save(output, format=\"PNG\")\n",
    "            image_string = output.getvalue()\n",
    "            output.close()\n",
    "            image = tf.compat.v1.Summary.Image(\n",
    "                        height=height, width=width, colorspace=channel,\n",
    "                        encoded_image_string=image_string)\n",
    "            value = tf.compat.v1.Summary.Value(tag=tag, image=image)\n",
    "            values.append(value)\n",
    "\n",
    "        summary = tf.compat.v1.Summary(value=values)\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctionAgent(FNAgent):\n",
    "\n",
    "    def save(self, model_path):\n",
    "        joblib.dump(self.model, model_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = joblib.load(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def initialize(self, experiences):\n",
    "        #　ここでスケールを\n",
    "        scaler = StandardScaler()\n",
    "        estimator = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=1)\n",
    "        self.model = Pipeline([(\"scaler\", scaler), (\"estimator\", estimator)])\n",
    "\n",
    "        #　経験からstateを抜き取る（vstackにexoeriencesから抜き取ったsを）\n",
    "        states = np.vstack([e.s for e in experiences])\n",
    "        #んで学習\n",
    "        self.model.named_steps[\"scaler\"].fit(states)\n",
    "\n",
    "        # Avoid the predict before fit.\n",
    "        self.update([experiences[0]], gamma=0)\n",
    "        self.initialized = True\n",
    "        print(\"Done initialization. From now, begin training!\")\n",
    "\n",
    "    def estimate(self, s):\n",
    "        estimated = self.model.predict(s)[0]\n",
    "        return estimated\n",
    "\n",
    "    def _predict(self, states):\n",
    "        if self.initialized:\n",
    "            predicteds = self.model.predict(states)\n",
    "\n",
    "        #モデルが初期化されていない，つまりまだモデルの準備ができていないとしても何かしらの予測値としてランダムに\n",
    "        #出力することができる\n",
    "        else:\n",
    "            size = len(self.actions) * len(states)\n",
    "            predicteds = np.random.uniform(size=size)\n",
    "            predicteds = predicteds.reshape((-1, len(self.actions)))\n",
    "            print(\"non predictation\")\n",
    "        return predicteds\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        states = np.vstack([e.s for e in experiences])\n",
    "        n_states = np.vstack([e.n_s for e in experiences])\n",
    "\n",
    "        estimateds = self._predict(states)\n",
    "        future = self._predict(n_states)\n",
    "\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                #ここで実装するのはoff-policyになっているので価値が最大になる行動をとることを前提とする\n",
    "                reward += gamma * np.max(future[i])\n",
    "\n",
    "            estimateds[i][e.a] = reward\n",
    "\n",
    "        estimateds = np.array(estimateds)\n",
    "        states = self.model.named_steps[\"scaler\"].transform(states)\n",
    "        self.model.named_steps[\"estimator\"].partial_fit(states, estimateds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleObserver(Observer):\n",
    "\n",
    "    def transform(self, state):\n",
    "        return np.array(state).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctionTrainer(Trainer):\n",
    "\n",
    "    def train(self, env, episode_count=220, epsilon=0.1, initial_count=-1,\n",
    "              render=False):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = ValueFunctionAgent(epsilon, actions)\n",
    "        self.train_loop(env, agent, episode_count, initial_count, render)\n",
    "        return agent\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        agent.initialize(self.experiences)\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            agent.update(batch, self.gamma)\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        rewards = [e.r for e in self.get_recent(step_count)]\n",
    "        self.reward_log.append(sum(rewards))\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(play):\n",
    "    env = CartPoleObserver(gym.make(\"CartPole-v0\"))\n",
    "    trainer = ValueFunctionTrainer()\n",
    "    path = trainer.logger.path_of(\"value_function_agent.pkl\")\n",
    "\n",
    "    if play:\n",
    "        agent = ValueFunctionAgent.load(env, path)\n",
    "        agent.play(env)\n",
    "    else:\n",
    "        trained = trainer.train(env)\n",
    "        trainer.logger.plot(\"Rewards\", trainer.reward_log,\n",
    "                            trainer.report_interval)\n",
    "        trained.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
