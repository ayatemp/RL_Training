{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras as K\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\",\n",
    "                        [\"s\", \"a\", \"r\", \"n_s\", \"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNAgent():\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "\n",
    "    def save(self, model_path):\n",
    "        self.model.save(model_path, overwrite=True, include_optimizer=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = K.models.load_model(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def initialize(self, experiences):\n",
    "        raise NotImplementedError(\"You have to implement initialize method.\")\n",
    "\n",
    "    def estimate(self, s):\n",
    "        raise NotImplementedError(\"You have to implement estimate method.\")\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        raise NotImplementedError(\"You have to implement update method.\")\n",
    "\n",
    "    def policy(self, s):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(s)\n",
    "            if self.estimate_probs:\n",
    "                action = np.random.choice(self.actions,\n",
    "                                          size=1, p=estimates)[0]\n",
    "                return action\n",
    "            else:\n",
    "                return np.argmax(estimates)\n",
    "\n",
    "    def play(self, env, episode_count=5, render=True):\n",
    "        for e in range(episode_count):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                a = self.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                episode_reward += reward\n",
    "                s = n_state\n",
    "            else:\n",
    "                print(\"Get reward {}.\".format(episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, buffer_size=1024, batch_size=32,\n",
    "                 gamma=0.9, report_interval=10, log_dir=\"\"):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.logger = Logger(log_dir, self.trainer_name)\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "\n",
    "    @property\n",
    "    def trainer_name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        snaked = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", class_name)\n",
    "        snaked = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", snaked).lower()\n",
    "        snaked = snaked.replace(\"_trainer\", \"\")\n",
    "        return snaked\n",
    "\n",
    "    def train_loop(self, env, agent, episode=200, initial_count=-1,\n",
    "                   render=False, observe_interval=0):\n",
    "        self.experiences = deque(maxlen=self.buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        frames = []\n",
    "\n",
    "        for i in range(episode):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.episode_begin(i, agent)\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if self.training and observe_interval > 0 and\\\n",
    "                   (self.training_count == 1 or\n",
    "                    self.training_count % observe_interval == 0):\n",
    "                    frames.append(s)\n",
    "\n",
    "                a = agent.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                e = Experience(s, a, reward, n_state, done)\n",
    "                self.experiences.append(e)\n",
    "                if not self.training and \\\n",
    "                   len(self.experiences) == self.buffer_size:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                self.step(i, step_count, agent, e)\n",
    "\n",
    "                s = n_state\n",
    "                step_count += 1\n",
    "            else:\n",
    "                self.episode_end(i, step_count, agent)\n",
    "\n",
    "                if not self.training and \\\n",
    "                   initial_count > 0 and i >= initial_count:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                if self.training:\n",
    "                    if len(frames) > 0:\n",
    "                        self.logger.write_image(self.training_count,\n",
    "                                                frames)\n",
    "                        frames = []\n",
    "                    self.training_count += 1\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        pass\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        pass\n",
    "\n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False\n",
    "\n",
    "    def get_recent(self, count):\n",
    "        recent = range(len(self.experiences) - count, len(self.experiences))\n",
    "        return [self.experiences[i] for i in recent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "\n",
    "    def render(self):\n",
    "        self._env.render(mode=\"human\")\n",
    "\n",
    "    def step(self, action):\n",
    "        n_state, reward, done, info = self._env.step(action)\n",
    "        return self.transform(n_state), reward, done, info\n",
    "\n",
    "    def transform(self, state):\n",
    "        raise NotImplementedError(\"You have to implement transform method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir=\"\", dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "        if not log_dir:\n",
    "            self.log_dir = os.path.join(os.path.dirname(__file__), \"logs\")\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.mkdir(self.log_dir)\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.compat.v1.keras.callbacks.TensorBoard(\n",
    "                            self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def path_of(self, file_name):\n",
    "        return os.path.join(self.log_dir, file_name)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.compat.v1.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_image(self, index, frames):\n",
    "        # Deal with a 'frames' as a list of sequential gray scaled image.\n",
    "        last_frames = [f[:, :, -1] for f in frames]\n",
    "        if np.min(last_frames[-1]) < 0:\n",
    "            scale = 127 / np.abs(last_frames[-1]).max()\n",
    "            offset = 128\n",
    "        else:\n",
    "            scale = 255 / np.max(last_frames[-1])\n",
    "            offset = 0\n",
    "        channel = 1  # gray scale\n",
    "        tag = \"frames_at_training_{}\".format(index)\n",
    "        values = []\n",
    "\n",
    "        for f in last_frames:\n",
    "            height, width = f.shape\n",
    "            array = np.asarray(f * scale + offset, dtype=np.uint8)\n",
    "            image = Image.fromarray(array)\n",
    "            output = io.BytesIO()\n",
    "            image.save(output, format=\"PNG\")\n",
    "            image_string = output.getvalue()\n",
    "            output.close()\n",
    "            image = tf.compat.v1.Summary.Image(\n",
    "                        height=height, width=width, colorspace=channel,\n",
    "                        encoded_image_string=image_string)\n",
    "            value = tf.compat.v1.Summary.Value(tag=tag, image=image)\n",
    "            values.append(value)\n",
    "\n",
    "        summary = tf.compat.v1.Summary(value=values)\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
