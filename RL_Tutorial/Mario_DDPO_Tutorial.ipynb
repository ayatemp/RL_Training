{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym-super-mario-bros==7.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! apt update\n",
    "# ! apt install xvfb\n",
    "# ! pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.26.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gym==0.26.2) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gym==0.26.2) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gym==0.26.2) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install gym==0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <3EF8D74C-9F11-3C9E-85DB-9E76BCCBE7A0> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <598DDE6B-5A2E-3301-B9C5-9034AEC256A9> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gymは、Open AIのRL用ツールキットです\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# OpenAI Gym用に使うNES エミュレーター\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#OpenAI Gymのスーパー・マリオ・ブラザーズの環境\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習：RLの各種用語整理\n",
    "環境：\n",
    "エージェントが相互作用をする対象であり、また、エージェントが学習を行う対象でもある世界です。\n",
    "\n",
    "行動 \n",
    " :\n",
    "エージェントが環境に対して行う働きかけです。\n",
    "エージェントが取りうる全ての行動の集合を「行動空間」と呼びます。\n",
    "\n",
    "状態 \n",
    "：\n",
    "環境の現在の状況です。\n",
    "環境が成り得る全ての状態の集合を「状態空間」と呼びます。\n",
    "\n",
    "報酬 \n",
    "：\n",
    "報酬は、環境からエージェントに対するフィードバックです。\n",
    "この報酬によってエージェントは学習し、行動選択を変えていきます。\n",
    "複数のタイムステップにわたる報酬の集合は、利得と呼ばれます。\n",
    "\n",
    "行動価値関数 \n",
    " :\n",
    " は、状態 \n",
    "の時にエージェントが任意の行動\n",
    "を取り、さらに将来のタイムステップにわたって利得を最大化する行動を取った場合の、利得の期待値を返す関数です。\n",
    "すなわち、\n",
    " はある状態 \n",
    "での行動\n",
    "の「質」を表していると言えます。\n",
    "この関数を近似した関数を作成することで、次の行動の「質」を評価して行動選択が行えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import nes_py\n",
    "import pkg_resources\n",
    "\n",
    "# print(f\"gym version: {gym.__version__}\")\n",
    "# print(f\"gym_super_mario_bros version: {pkg_resources.get_distribution('gym_super_mario_bros').version}\")\n",
    "# print(f\"nes_py version: {nes_py.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 20:22:34.799 Python[38440:7370589] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/s_/w4ncchnj77322v1x6dslwsq40000gn/T/org.python.python.savedState\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# スーパー・マリオの環境を初期化\n",
    "# env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\")\n",
    "\n",
    "# 行動空間を以下に制限\n",
    "#   0. 右に歩く\n",
    "#   1. 右方向にジャンプ\n",
    "env = gym.make('SuperMarioBros-v1', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, truncated, info = env.step(action = 0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.step の説明\n",
    "\n",
    "### 返り値\n",
    "\n",
    "- `obs`: (240, 256, 3)\n",
    "  - 240×256の3チャンネルで画面を作成している\n",
    "- `reward`: float型\n",
    "- `done`: bool型\n",
    "- `truncated`: bool型\n",
    "- `info`: dict型\n",
    "\n",
    "### info の内容\n",
    "\n",
    "| キー | 値 | 説明 |\n",
    "|------|-----|------|\n",
    "| 'coins' | 0 | 現在のコイン数 |\n",
    "| 'flag_get' | False | フラッグ（ゴール）に到達していない |\n",
    "| 'life' | 2 | 残りライフ数 |\n",
    "| 'score' | 0 | 現在のスコア |\n",
    "| 'stage' | 1 | 現在のステージ |\n",
    "| 'status' | 'small' | マリオの状態（小さい状態） |\n",
    "| 'time' | 400 | 残り時間 |\n",
    "| 'world' | 1 | 現在のワールド |\n",
    "| 'x_pos' | 40 | ゲーム内でのX座標位置 |\n",
    "| 'x_pos_screen' | 40 | 画面上でのX座標位置 |\n",
    "| 'y_pos' | 79 | Y座標位置 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29fcb49a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今の所，RGB値はAgentにとって必要のないものなのでこれをラッパーしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"スキップした後のフレームのみを返す\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"行動を繰り返し、報酬を合計する\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # 報酬を蓄積し、同じ行動を繰り返す\n",
    "            # next_state, reward, done, info = self.env.step(action)\n",
    "            # result = self.env.step(action)\n",
    "            # print(f\"env.step() returned {len(result)} values: {result}\")\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "# グレースケールに変換するラッパー\n",
    "# 変更後　next_state.shape: (240, 256, 3) -> (1, 240, 256)\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # [H, W, C] のarrayを、[C, H, W] のtensorに変換\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# 環境にWrapperを適用\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mario:\n",
    "#     def __init__():\n",
    "#         pass\n",
    "\n",
    "#     def act(self, state):\n",
    "#         \"\"\"状態が与えられたとき、ε-greedy法に従って行動を選択します\"\"\"\n",
    "#         pass\n",
    "\n",
    "#     def cache(self, experience):\n",
    "#         \"\"\"経験をメモリに追加します\"\"\"\n",
    "#         pass\n",
    "\n",
    "#     def recall(self):\n",
    "#         \"\"\"記憶からの経験のサンプリングします\"\"\"\n",
    "#         pass\n",
    "\n",
    "#     def learn(self):\n",
    "#         \"\"\"経験のデータのバッチで、オンラインに行動価値関数(Q)を更新します\"\"\"\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marioの動きの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarioのAction定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # 最適な行動を予測するマリオ用のDNN「訓練」セクションで実装\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  #  Netを保存するまでの実験ステップの数です\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        状態が与えられると、ε-greedy法で行動を選択し、ステップの値を更新\n",
    "\n",
    "        Inputs:\n",
    "            state(LazyFrame):現在の状態における一つの観測オブジェクトで、(state_dim)次元となる\n",
    "        Outputs:\n",
    "            action_idx (int): マリオが取る行動を示す整数値\n",
    "        \"\"\"\n",
    "        # 探索（EXPLORE）\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # 活用（EXPLOIT）\n",
    "        else:\n",
    "            # 入力された状態をnupmy配列に変換\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            # 最適な行動を選択\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # exploration_rateを減衰させます\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # ステップを+1します\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記憶の保存と呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # さきほどのクラスのサブクラス\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        経験をself.memory (replay buffer)に保存\n",
    "\n",
    "        Inputs:\n",
    "            state (LazyFrame),\n",
    "            next_state (LazyFrame),\n",
    "            action (int),\n",
    "            reward (float),\n",
    "            done(bool))\n",
    "        \"\"\"\n",
    "        print(type(state), state)\n",
    "        # state = state.__array__()\n",
    "        # next_state = next_state.__array__()\n",
    "        state = np.array(state[0])\n",
    "        next_state = np.array(next_state[0])\n",
    "\n",
    "        # GPUを使うかどうか\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        メモリから経験のバッチを取得します\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練\n",
    "\n",
    "マリオの強化学習アルゴリズムには、DDQNアルゴリズムを使用します。\n",
    "\n",
    "#### DDQNの特徴\n",
    "\n",
    "DDQNでは、以下の2つのConvNetを使用します：\n",
    "\n",
    "1. Q_online\n",
    "2. Q_target\n",
    "\n",
    "これらのConvNetは、それぞれ独立して最適な行動価値関数を近似します。\n",
    "\n",
    "#### 実装の詳細\n",
    "\n",
    "本実装では：\n",
    "\n",
    "- Q_online と Q_target は同じ特徴生成器を使用\n",
    "- ただし、分類器としては別々に更新される\n",
    "\n",
    "#### パラメータの更新方法\n",
    "\n",
    "1. Q_target のパラメータ：\n",
    "   - 逆伝播の際に更新されないよう、固定される\n",
    "\n",
    "2. Q_target の更新方法：\n",
    "   - Q_online の値と定期的に同期される\n",
    "\n",
    "このように、Q_target は間接的に更新されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"\n",
    "    単純なCNN構造とし、以下の通りです\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "\n",
    "    DDQNでは行動の価値がオンラインとターゲットの二つで計算される\n",
    "    まず最初にオンラインネットワークで行動を選択\n",
    "    次にターゲットネットワークで行動の価値を計算\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        # inputの次元をチェック\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target のパラメータは固定されます\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD 推論値とTDターゲット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 具体的な学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # 経験を訓練させるために最低限必要なステップ数\n",
    "        self.learn_every = 3  # Q_onlineを更新するタイミングを示すステップ数\n",
    "        self.sync_every = 1e4  # Q_target & Q_onlineを同期させるタイミングを示すステップ数\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # メモリからサンプリング\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # TD Estimateの取得\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # TD Targetの取得\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # 損失をQ_onlineに逆伝播させる\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ログの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # 指標の履歴\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # reacord()が呼び出されるたびに追加される移動平均\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # 現在のエピソードの指標\n",
    "        self.init_episode()\n",
    "\n",
    "        # 時間を記録\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"エピソード終了時の記録\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際にやってみた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "0\n",
      "<class 'tuple'> (<gym.wrappers.frame_stack.LazyFrames object at 0x29fd91990>, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute '__array__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 記憶(truncatedは抜く)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 訓練\u001b[39;00m\n\u001b[1;32m     31\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m mario\u001b[38;5;241m.\u001b[39mlearn()\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mMario.cache\u001b[0;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m経験をself.memory (replay buffer)に保存\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    done(bool))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(state), state)\n\u001b[0;32m---> 19\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array__\u001b[49m()\n\u001b[1;32m     20\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39m__array__()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# GPUを使うかどうか\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '__array__'"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 105 # 元々は10でしたが、日本語版では少し伸ばしてみましょう。5分程度かかります\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # ゲーム開始！\n",
    "    while True:\n",
    "\n",
    "        # 現在の状態に対するエージェントの行動を決める\n",
    "        action = mario.act(state)\n",
    "        print(action)\n",
    "\n",
    "        # エージェントが行動を実行\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # 記憶(truncatedは抜く)\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 訓練\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # ログ保存\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 状態の更新\n",
    "        state = next_state\n",
    "\n",
    "        # ゲームが終了したかどうかを確認\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
